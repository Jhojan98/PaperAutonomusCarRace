\section{Introduction}
% 1 page. Context, challenges, prior work (cite using [1], [2]).
Las carreras autónomas se han consolidado como un banco de pruebas exigente y revelador para la tecnología de conducción autónoma. Un circuito cerrado obliga a los sistemas de percepción y a los controladores en lazo cerrado a operar en su límite: el vehículo debe negociar curvas pronunciadas a velocidades sostenidas, mantener tolerancias laterales de centímetros y reaccionar en milisegundos a cambios en la superficie. Por ello, el rendimiento en un autódromo se considera un indicador temprano del progreso del sector, pues los métodos que superan estas exigencias suelen trasladarse con mínimas modificaciones a entornos urbanos de menor velocidad y mayor margen de seguridad.

Los primeros trabajos emplearon estrategias diseñadas a mano, como máquinas de estados finitos o control predictivo basado en el modelo cinemático de bicicleta. El MPC ofrece garantías matemáticas—por ejemplo, la permanencia dentro de los límites de la pista—pero depende de una optimización en línea precisa y de una identificación fiel del sistema. Su desempeño se degrada cuando la fricción neumático-pista, la distribución de masas o la geometría del trazado difieren de los valores nominales. Investigaciones posteriores añadieron linealización por realimentación o funciones barrera de control discretas para imponer restricciones de seguridad, pero la especificación manual de dichas restricciones volvió a convertirse en cuello de botella cuando el ruido de percepción dominó el error en lazo cerrado.

El aprendizaje por refuerzo profundo (DRL) propone una alternativa guiada por datos que aprende percepción y control de forma conjunta. El entorno CarRacing-v0 de OpenAI Gym demostró que una red DQN convolucional, alimentada con píxeles crudos, es capaz de completar pistas generadas proceduralmente. Extensiones como doble DQN, cabezas duela y reproducción priorizada estabilizaron el aprendizaje, mientras que las variantes de acción continua reemplazaron la cuadrícula discreta de acelerador-freno por políticas gaussianas parametrizadas. La plataforma DeepRacer de Amazon trasladó estas ideas a coches físicos a escala 1/18 y evidenció que los métodos de gradiente de política—en particular PPO—logran seguimiento robusto de carril y evitación de obstáculos en bucles de entrenamiento de realidad mixta. A mayor escala, la comunidad F1TENTH comparó agentes DRL fuera de línea entrenados con demostraciones expertas para reducir la costosa interacción real, y el reto Learn-to-Race evaluó la generalización entre dominios en simuladores fotorrealistas.

Aun con estos avances, persisten tres retos. (i) Riqueza perceptual: la mayoría de los agentes DRL se limitan a haces LiDAR escasos o flujos RGB monoculares, ignorando señales informativas como ángulos de derrape, aceleraciones verticales o gradientes de temperatura de neumáticos, esenciales cerca del límite de adherencia. (ii) Escasez y no estacionariedad de la recompensa: los términos de shaping ajustados para un único circuito rara vez incentivan comportamientos deseables como la modulación eficiente del acelerador o la entrada suave en curvas, y pueden volverse engañosos cuando el agente domina el mantenimiento básico de carril. (iii) Eficiencia muestral: incluso los agentes actor-crítico más modernos requieren millones de fotogramas de simulación, lo que dificulta su aplicación directa en hardware físico.

Este artículo aborda dichas brechas mediante (1) el doble de resolución LiDAR fusionada con características inerciales mediante un codificador basado en atención, (2) un mecanismo adaptativo de shaping que reajusta penalizaciones y bonificaciones según la varianza reciente del error de trayectoria, y (3) una convergencia acelerada gracias a reproducción priorizada distribuida implementada en Ray RLlib. Sobre la base de simulaciones de modelo de bicicleta cinemática extendida empleadas en nuestros análisis preliminares, demostramos que la arquitectura propuesta reduce el tiempo medio de vuelta en 23 \% y los incidentes fuera de pista en 67 \% en CarRacing-v0, transfiriéndose sin reajuste al circuito “Austria” de F1TENTH.

El resto del artículo se organiza así: la Sección 2 formaliza el problema y repasa la dinámica vehicular; la Sección 3 detalla la percepción multimodal y el diseño de recompensa adaptativa; la Sección 4 describe la tubería de entrenamiento distribuido y el protocolo de evaluación; la Sección 5 presenta resultados experimentales y estudios de ablación; y la Sección 6 concluye con líneas futuras de investigación.
