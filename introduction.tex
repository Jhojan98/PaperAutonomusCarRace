\section{Introduction}
% 1 page. Context, challenges, prior work (cite using [1], [2]).

Autonomous control of vehicles has become a benchmark problem in reinforcement learning, but reproducing real‐world driving in the laboratory often requires complex simulators or physical testbeds. In this project, we simplify the task by focusing on the Gymnasium CarRacing-v0 environment \cite{gymnasium2023}, where a planar racing car must follow a procedurally generated track using only on‐board sensors. The initial problem we address is thus configuring a virtual racing car to drive through a circuit—rather than dealing with real vehicles or full 3D simulators—so that we can concentrate on core challenges of perception, action selection, and reward design in a lightweight Box2D physics setting \cite{gymnasium2023}. By adopting CarRacing-v0, which already handles chassis joints, tire friction, and steering dynamics internally, we avoid reinventing vehicle dynamics and can direct our efforts toward improving the learning algorithm itself.

In particular, several recent works focus on the Gymnasium CarRacing-v0 environment, providing step-by-step DQN implementations and baseline performance metrics that serve as a foundation for further improvements \cite{perod2019}. By analyzing these implementations—including the detailed project report by Perod et al. \cite{perod2019}—we identified opportunities to enhance observation processing and reward shaping.


In this work, we present a DQN-based agent for the CarRacing environment in the Gymnasium library \cite{gymnasium2023}. This environment uses the Box2D physics engine to solve the planar rigid–body equations
\[
m\dot v = \sum F,\quad I\dot \omega = \sum \tau,
\]
and handles chassis joints, tire friction, and steering internally. By building on this existing model, we avoid implementing vehicle dynamics from scratch and focus instead on the learning algorithm.

To improve the agent’s perception, we augment the standard RGB input with a simulated 14-ray LiDAR sensor mounted at the front of the vehicle. The LiDAR readings provide a vector of distances to nearby obstacles, giving the agent direct information about track geometry before it appears in the camera view. Inspired by prior work on obstacle detection and autonomous driving \cite{Estrada2021}, our agent processes both image frames and LiDAR data through parallel neural network branches that merge before the Q-value output layer.

We evaluate our approach by training the agent on a set of standard CarRacing tracks and then testing it on unseen layouts. Our results indicate that adding LiDAR feedback leads to faster convergence during training and more stable driving behavior on new tracks. This suggests that combining visual and range‐based observations can help DQN agents generalize better in driving tasks.

This work is presented as a small university research project in which we will apply concepts seen in class to a practical driving task. We will use ideas from cybernetics to understand how the car can sense and correct its motion, feedback loops to adjust steering and speed based on the track ahead, and control agents to model the decision maker that chooses actions at each time step. Our main contribution is to build a DQN agent that combines image input and a simulated 14‐ray LiDAR sensor, showing that this multimodal approach helps the car learn faster and drive more smoothly on new tracks. We also design a hybrid reward function that balances speed and staying centered on the road, and we release all code and test tracks to support reproducibility.

The rest of the document is organized as follows. In Section Methods and Materials we explain the environment, the network architecture, and how we simulate the LiDAR rays. In Section Results we report training curves, evaluation scores on unseen tracks, and qualitative driving examples. Also we discuss how cybernetic principles and feedback loops influenced our design choices, and we compare our results to baseline DQN implementations. Finally, in Section conclusiones we summarize our findings and suggest future directions for improving control agents in lightweight driving simulators.